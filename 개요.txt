문제 유형

세가지 주제의 문제가 나왔고 각 큰 단위 문제 안에 세부적으로 3~4문제씩 세부문제 출제

데이터 파일의 개수 및 크기 : sales.csv, risk_proj.txt, locations.txt 10KB미만, blog.txt 약 6메가
[출처] ADP] 제6회 ADP 실기 시험 후기(2016.4.30)|작성자 수제비



1. 통계학 기반 분석 (40점)

   - 각 설명변수들과 출산률(종속변수)의 관계를 회귀분석으로 정의 및 결과를 해석하는 문제  
   - 'Data 전처리', 'Model 생성', '분석 Model별 검증', '결과 해석'
   - 회귀문제는 feature engineering과 관련이 깊다. 어떤 변수를 어떻게 사용할것인지, 
   - 그리고 서로 다중공선성의 문제는 없는지.. 뭐 이런것들을 좀 나열해야한다. 
   - 선형회귀란것이 하나의 아웃라이어에도 많은 영향을 받으며, 
   - 훈련 변수들끼리 상관이 높다면 결과가 이상하게 나오는 등 문제가 많기 때문에 이를 검증하기에 적당한것같다. 
dplyr ggplot2


2. Text mining (20점)

   - 영화평 Data를 전처리 후, '형용사'를 추출(예, SimplePos22 함수) 하여 감성 분석 하는 문제
   - Text 전처리, KoNLP 패키지를 다양한 방식으로 사용해본 경험
   - Text Data(Text file이였음)를 불러오는 부분에서 구분자를 가지고 Parsing을 하는데 문제가 발생
   - 파일 용량 자체가 커서 한번 불러오는데 분단위 시간이 걸림
텍스트마이닝은 점수배점이제일 작은것으로 느껴졌다. 이정도 되면 시간이 없을텐데 일단 파일을 불러와 빈도수만 체크하는걸로 일단은 만족하며 제출하는편이 나을것같다.


3. Data Mining/ML (40점)

   - (R을 공부하는 많은 사람들이 익숙한) 타이타닉 생존자 Data를 Data mining 학습하여, 생존여부 예측을 하는 문제 
   - 금번회차의 경우 분석 과정은 상관없고,  오직 제공된 Test Data의 정답만을 제출해서, 예측한 정답의 적중률이 얼마나 높은지로 채점을 함
   - R을 이용한 Data처리 & 분석실무 (길벗)

데이터마이닝 문제에서는 사실 피처를 빼고 말고가 중요하지 않다. 
최대한 빠르게 훈련이 가능한 형태로 데이터를 바꿔야 하며, 
예를들어 ID가 여러번 등장할경우는 하나로 통일을 해서 훈련데이터를 미리 마사지 해놓는 짓이 필요하다. 
라이브러리를 그냥 호출하는건 너무나도 쉽기에 
R이라면 reshape2를 능숙하게 사용해서 wide <-> long데이터간 변환을 자유자재로 하면 시간을 단축할수 있을것같다. 
파이썬의 경우는 좀더 일반언어와 가깝기 때문에 판다스 그룹바이등과 같은 aggregate이나 람다식을 이용하면 좀더 빠르게 코딩이 가능하나, 
반복문에 대한 이해가 좀 있어야 하기 때문에 이해가 어려울수 있다
glm / random forest



9. 제6회 ADP 실기 기출 문제 복기
 문제1. 데이터 전처리 및 데이터 마이닝(R로 QF(?) 알고리듬을 직접 구현하여 단계별로 만들어 보는 문제)
- 데이터
sales.csv 5천건
cust_id,prod_id,amt
1,prod01,100
1,prod02,200 
11,prod02,300
21,prod03,100 
...

(가) sales.csv를 읽어서 cust_id 및 prod_id별로 amt가 0보다 크면 1, 아니면 0으로 아래 처럼 만들 것
cust_id prod01  prod02  prod03  prod04  prod05  prod06 ...
      1     1      0        1       1         0        0

      11    0       1        0       0         0        0

      21    0        0        1       1         1       0

      31    1       1        0        1        1       1

...

(나) 위의 결과를 이용하여 cust_id 별로 상호 인접정도를 파악하기위해 cust_id별로 피어슨상관계수 행열을 아래와 같은 모양으로 만들 것

    1    11     21     31...

1  1.00  0.72   0.61    0.83

11 0.72  1.00   0.75    0.88

21 0.61  0.75   1.00    0.93

31 0.83  0.88   0.93    1.00

...

- (가)의 결과를 그냥 cor함수에 넣으면 오류남



(다) 특정 user의 유사도가 높은 15 user 구해서 다음 형태로 만들기? (행과 열이름은 cust_id였고 각 행렬의 값은 같은 상품을 구매한 amt 였던 것으로 추정됨)

   1  11 21 31...

1 1200 100 200 400

11 100 2300 500 900

21 200 500 1500 800

31 400 900 800 1000

...

(라) 특정 user 당 5개씩 추천 아이템 생성하기? (잘 기억 안남)

prod01,prod03 ,prod06....



2. 통계분석

- 데이터 : risk_proj.txt (성별,인종별,나이,활동성,risk)



(가) 성별에 따라 risk가 차이가 있는지 분석할 것

나이등 다른 변수와 교호작용 있는지 알아볼 것(가정사항등 정의)



(나) 인종별 risk가 차이가 있는지  분석

나이등 다른 변수와 교호작용 있는지 알아볼 것(가정사항등 정의)



(다) 인종및 성별에 따라 risk가 차이가 있는지 분석. 교호착용 분석



3. 텍스트 마이닝

- 데이터 : location.txt  (UTF-8, 헤더 없음)

  가평,ncn

  가야,ncn

  남이섬,ncn

   ....

- 데이터 : blog.txt  ( TAB문자로 구분됨)

  DATE      TITLE        CONTENT

  20150101 제목        봄관련 내용....

  ....



(가) 자료읽기

   (ㄱ) location.txt를 읽어서 사용자사전에 등록하기

   (ㄴ) blog.txt를 다음 형식으로 읽을 것

     DATE :  numeric

     TITLE : character

     CONTENT : character



(나) blog.txt에서 봄여행,벚꽃축제,봄나들이 등 봄과 관련된 문서만 추출하기

   (가)에서 읽은 사용자 사전에 들어있는 지명이 들어있는 문서만 추출

 

(다) 위에서 추출된 문서에 대해 명사추출 및 출현 빈도 10위 추출



(라) 봄과 관련된 지명 출현 빈도 10위까지 추출하여 시각화



10. 기존 기출문제 분석

- 일단 제1회,2회,3회 기출문제는 제 블로그 아래에 하나로 정리해두었습니다. 

  http://blog.naver.com/sujebee/220694267110

  4,5회 기출문제는 아무리 검색을 해봐도 찾을 수가 없었습니다.

- 먼저 기존 기출문제들을 보고 오시면 출제 경향에 대한 감을 잡을 수 있습니다.

- 1회 시험은 아직 시험이 현재와 같은 틀을 갖추기 이전에 출제된 것이라서 지금의 경향과 많은 차이가 있습니다.

  1회 시험은 그냥 이런 문제도 있었구나 수준으로만 보면 됩니다.

- 기존 출제 영역 : 데이터전처리, 데이터마이닝, 통계분석, 텍스트 마이닝

  1) 데이터 전처리(멍잉)

   - R 스크립트 작성 능력

   - ddply, aggregate, merge, melt/cast, sqldf 등을 이용한 처리

   - NA 결측치(imputation),이상치 처리.



  2) 데이터 마이닝 : 분류, 연관, 군집분석 수준 정도만 출제됨

   - 분류예측 : 해지여부 예측

   - 연관 : 로또번호분석

   - 군집 : 색상 스타일별 군집. 최적 k값 찾기

   - 변수 선택 능력 : nearZeroVar, findCorrelation, step(), PCA(princomp, prcomp, biplot)

 

  3) 통계 분석 능력 : 주로 분산분석(ANOVA),로지스틱분석 에서 출제됨

  - 교호작용, 잔차분석, 시각화 능력

  - 일원/이원분산분석  : aov(), 결측치, 이상치 고려



  4) 텍스트 마이닝 : 주로 KoNLP를 이용한 빈도 분석
[출처] ADP] 제6회 ADP 실기 시험 후기(2016.4.30)|작성자 수제비

